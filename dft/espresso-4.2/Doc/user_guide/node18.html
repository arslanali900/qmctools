<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>3.3 Parallelization levels</TITLE>
<META NAME="description" CONTENT="3.3 Parallelization levels">
<META NAME="keywords" CONTENT="user_guide">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="user_guide.css">

<LINK REL="next" HREF="node19.html">
<LINK REL="previous" HREF="node17.html">
<LINK REL="up" HREF="node15.html">
<LINK REL="next" HREF="node19.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html547"
  HREF="node19.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.gif"></A> 
<A NAME="tex2html543"
  HREF="node15.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.gif"></A> 
<A NAME="tex2html537"
  HREF="node17.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.gif"></A> 
<A NAME="tex2html545"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html548"
  HREF="node19.html">3.4 Tricks and problems</A>
<B> Up:</B> <A NAME="tex2html544"
  HREF="node15.html">3 Parallelism</A>
<B> Previous:</B> <A NAME="tex2html538"
  HREF="node17.html">3.2 Running on parallel</A>
 &nbsp; <B>  <A NAME="tex2html546"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><UL>
<LI><A NAME="tex2html549"
  HREF="node18.html#SECTION00043010000000000000">3.3.0.1 Massively parallel calculations</A>
</UL>
<BR>
<LI><A NAME="tex2html550"
  HREF="node18.html#SECTION00043100000000000000">3.3.1 Understanding parallel I/O</A>
<UL>
<LI><A NAME="tex2html551"
  HREF="node18.html#SECTION00043110000000000000">3.3.1.1 Cray XT3</A>
</UL></UL>
<!--End of Table of Child-Links-->
<HR>

<H2><A NAME="SECTION00043000000000000000">
3.3 Parallelization levels</A>
</H2>

<P>
Data structures are distributed across processors.
Processors are organized in a hierarchy of groups, 
which are identified by different MPI communicators level.
The groups hierarchy is as follow:
<PRE>
                 /  pools _ task   groups
  world _ images
                 \ linear-algebra  groups
</PRE>

<P>
<B>world</B>: is the group of all processors (MPI_COMM_WORLD).

<P>
<B>images</B>: Processors can then be divided into different "images",
corresponding to a point in configuration space (i.e. to
a different set of atomic positions). Such partitioning 
is used when performing Nudged Elastic band (NEB) calculations.

<P>
<B>pools</B>: When k-point sampling is used, each image group can be 
subpartitioned into "pools", and k-points can distributed to pools.
Within each pool, reciprocal space basis set (PWs)
and real-space grids are distributed across processors.
This is usually referred to as "PW parallelization".
All linear-algebra operations on array of  PW / 
real-space grids are automatically and effectively parallelized.
3D FFT is used to transform electronic wave functions from
reciprocal to real space and vice versa. The 3D FFT is
parallelized by distributing planes of the 3D grid in real
space to processors (in reciprocal space, it is columns of
G-vectors that are distributed to processors). 

<P>
<B>task groups</B>: 
In order to allow good parallelization of the 3D FFT when 
the number of processors exceeds the number of FFT planes,
data can be redistributed to "task groups" so that each group 
can process several wavefunctions at the same time.

<P>
<B>linear-algebra group</B>:
A further level of parallelization, independent on
PW or k-point parallelization, is the parallelization of
subspace diagonalization (.x) or iterative orthonormalization
(.x). Both operations required the diagonalization of 
arrays whose dimension is the number of Kohn-Sham states
(or a small multiple). All such arrays are distributed block-like
across the ``linear-algebra group'', a subgroup of the pool of processors,
organized in a square 2D grid. As a consequence the number of processors
in the linear-algebra group is given by <I>n</I><SUP>2</SUP>
<tex2html_verbatim_mark>, where <I>n</I>
<tex2html_verbatim_mark> is an integer;
<I>n</I><SUP>2</SUP>
<tex2html_verbatim_mark> must be smaller than the number of precessors of a single pool.
The diagonalization is then performed
in parallel using standard linear algebra operations.
(This diagonalization is used by, but should not be confused with,
the iterative Davidson algorithm). One can choose to compile
ScaLAPACK if available, internal built-in algorithms otherwise.

<P>
<B>Communications</B>:
Images and pools are loosely coupled and processors communicate
between different images and pools only once in a while, whereas
processors within each pool are tightly coupled and communications
are significant. This means that Gigabit ethernet (typical for
cheap PC clusters) is ok up to 4-8 processors per pool, but <EM>fast</EM>
communication hardware (e.g. Mirynet or comparable) is absolutely 
needed beyond 8 processors per pool.

<P>
<B>Choosing parameters</B>:
To control the number of processors in each group,
command line switches: <TT>-nimage</TT>, <TT>-npools</TT>,
<TT>-ntg</TT>, <TT>northo</TT> (for .x) or <TT>-ndiag</TT>
(for .x) are used.
As an example consider the following command line:
<PRE>
mpirun -np 4096 ./pw.x -nimage 8 -npool 2 -ntg 8 -ndiag 144 -input my.input
</PRE>
This executes <TT>PWscf</TT> on 4096 processors, to simulate a system
with 8 images, each of which is distributed across 512 processors.
k-points are distributed across 2 pools of 256 processors each, 
3D FFT is performed using 8 task groups (64 processors each, so
the 3D real-space grid is cut into 64 slices), and the diagonalization
of the subspace Hamiltonian is distributed to a square grid of 144
processors (12x12).

<P>
Default values are: <TT>-nimage 1 -npool 1 -ntg 1</TT> ; 
<TT>ndiag</TT> is set to 1 if ScaLAPACK is not compiled,
it is set to the square integer smaller than or equal to  half the number 
of processors of each pool.

<P>

<H4><A NAME="SECTION00043010000000000000">
3.3.0.1 Massively parallel calculations</A>
</H4>
For very large jobs (i.e. O(1000) atoms or so) or for very long jobs
to be run on massively parallel  machines (e.g. IBM BlueGene) it is
crucial to use in an effective way both the "task group" and the
"linear-algebra" parallelization. Without a judicious choice of
parameters, large jobs will find a stumbling block in either memory or 
CPU requirements. In particular, the linear-algebra parallelization is
used in the diagonalization  of matrices in the subspace of Kohn-Sham
states (whose dimension is as a strict minumum equal to the number of
occupied states). These are stored as block-distributed matrixes
(distributed across processors) and diagonalized using custom-taylored
diagonalization algorithms that work on block-distributed matrixes.

<P>
Since v.4.1, ScaLAPACK can be used to diagonalize block distributed
matrixes, yielding better speed-up than the default algorithms for
large (&gt; 1000
<tex2html_verbatim_mark>) matrices, when using a large number of processors 
(&gt; 512
<tex2html_verbatim_mark>). If you want to test ScaLAPACK,
use <TT>configure -with-scalapack</TT>. This
will add
<TT>-D__SCALAPACK</TT> to DFLAGS in <TT>make.sys</TT> and set LAPACK_LIBS to something
like:
<PRE>
    LAPACK_LIBS = -lscalapack -lblacs -lblacsF77init -lblacs -llapack
</PRE>
The repeated <TT>-lblacs</TT> is not an error, it is needed! If <TT>configure</TT> does not recognize
ScaLAPACK, inquire with your system manager
on the correct way to link them.

<P>
A further possibility to expand scalability, especially on machines
like IBM BlueGene, is to use mixed MPI-OpenMP. The idea is to have
one (or more) MPI process(es) per multicore node, with OpenMP
parallelization inside a same node. This option is activated by  <TT>configure -with-openmp</TT>,
which adds preprocessing flag -D__OPENMP
and one  of the following compiler options:
<BLOCKQUOTE>
ifort: <TT>-openmp</TT>
<BR>
xlf:   <TT>-qsmp=omp</TT>
<BR>
PGI:   <TT>-mp</TT>
<BR>
ftn:   <TT>-mp=nonuma</TT>

</BLOCKQUOTE>
OpenMP parallelization is currently implemented and tested for the following combinations of FFTs
and libraries:
<BLOCKQUOTE>
internal FFTW copy: <TT>-D__FFTW</TT>
<BR>
ESSL: <TT>-D__ESSL</TT> or <TT>-D__LINUX_ESSL</TT>, link 
 with <TT>-lesslsmp</TT>
<BR>
ACML: <TT>-D__ACML</TT>, link with <TT>-lacml_mp</TT>.

</BLOCKQUOTE>
Currently, ESSL (when available) are faster than internal FFTW,
which in turn are faster than ACML.

<P>

<H3><A NAME="SECTION00043100000000000000">
3.3.1 Understanding parallel I/O</A>
</H3>
In parallel execution, each processor has its own slice of wavefunctions, 
to be written to temporary files during the calculation. The way wavefunctions 
are written by .x is governed by variable <TT>wf_collect</TT>, 
in namelist &amp;CONTROL 
If <TT>wf_collect=.true.</TT>, the final wavefunctions are collected into a single 
directory, written by a single processor, whose format is independent on 
the number of processors. If <TT>wf_collect=.false.</TT> (default) each processor
writes its own slice of the final 
wavefunctions to disk in the internal format used by <TT>PWscf</TT>. 

<P>
The former case requires more
disk I/O and disk space, but produces portable data files; the latter case
requires less I/O and disk space, but the data so produced can be read only
by a job running on the same number of processors and pools, and if
all files are on a file system that is visible to all processors
(i.e., you cannot use local scratch directories: there is presently no
way to ensure that the distribution of processes on processors will
follow the same pattern for different jobs).

<P>
.x instead always collects the final wavefunctions into a single directory.
Files written by .x can be read by .x only if <TT>wf_collect=.true.</TT> (and if
produced for <I>k</I> = 0
<tex2html_verbatim_mark> case). 
The directory for data is specified in input variables
<TT>outdir</TT> and <TT>prefix</TT> (the former can be specified
as well in environment variable ESPRESSO_TMPDIR):
<TT>outdir/prefix.save</TT>. A copy of pseudopotential files
is also written there. If some processor cannot access the
data directory, the pseudopotential files are read instead
from the pseudopotential directory specified in input data.
Unpredictable results may follow if those files
are not the same as those in the data directory!

<P>
<EM>IMPORTANT:</EM>
Avoid I/O to network-mounted disks (via NFS) as much as you can! 
Ideally the scratch directory <TT>outdir</TT> should be a modern 
Parallel File System. If you do not have any, you can use local
scratch disks (i.e. each node is physically connected to a disk
and writes to it) but you may run into trouble anyway if you 
need to access your files that are scattered in an unpredictable
way across disks residing on different nodes.

<P>
You can use input variable <TT>disk_io='minimal'</TT>, or even 
<TT>'none'</TT>, if you run
into trouble (or into angry system managers) with eccessive I/O with .x. 
The code will store wavefunctions into RAM during the calculation.
Note however that this will increase your memory usage and may limit 
or prevent restarting from interrupted runs.

<H4><A NAME="SECTION00043110000000000000">
3.3.1.1 Cray XT3</A>
</H4>
On the cray xt3 there is a special hack to keep files in
memory instead of writing them without changes to the code.
You have to do a: 
module load iobuf
before compiling and then add liobuf at link time.
If you run a job you set the environment variable 
IOBUF_PARAMS to proper numbers and you can gain a lot.
Here is one example:
<PRE>
env IOBUF_PARAMS='*.wfc*:noflush:count=1:size=15M:verbose,\
*.dat:count=2:size=50M:lazyflush:lazyclose:verbose,\
*.UPF*.xml:count=8:size=8M:verbose' pbsyod =\
\~{}/pwscf/pwscfcvs/bin/pw.x npool 4 in si64pw2x2x2.inp &gt; &amp; \
si64pw2x2x232moreiobuf.out &amp;
</PRE>
This will ignore all flushes on the *wfc* (scratch files) using a
single i/o buffer large enough to contain the whole file (<IMG
 WIDTH="19" HEIGHT="19" ALIGN="BOTTOM" BORDER="0"
 SRC="img2.gif"
 ALT="$ \sim$"> 12
<tex2html_verbatim_mark> Mb here).
this way they are actually never(!) written to disk.
The *.dat files are part of the restart, so needed, but you can be
'lazy' since they are writeonly. .xml files have a lot of accesses
(due to iotk), but with a few rather small buffers, this can be
handled as well. You have to pay attention not to make the buffers
too large, if the code needs a lot of memory, too and in this example
there is a lot of room for improvement. After you have tuned those
parameters, you can remove the 'verboses' and enjoy the fast execution.
Apart from the i/o issues the cray xt3 is a really nice and fast machine.
(Info by Axel Kohlmeyer, maybe obsolete)

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html547"
  HREF="node19.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.gif"></A> 
<A NAME="tex2html543"
  HREF="node15.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.gif"></A> 
<A NAME="tex2html537"
  HREF="node17.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.gif"></A> 
<A NAME="tex2html545"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html548"
  HREF="node19.html">3.4 Tricks and problems</A>
<B> Up:</B> <A NAME="tex2html544"
  HREF="node15.html">3 Parallelism</A>
<B> Previous:</B> <A NAME="tex2html538"
  HREF="node17.html">3.2 Running on parallel</A>
 &nbsp; <B>  <A NAME="tex2html546"
  HREF="node1.html">Contents</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
Paolo Giannozzi
2010-05-07
</ADDRESS>
</BODY>
</HTML>
